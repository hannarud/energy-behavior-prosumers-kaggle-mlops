{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook relevant version changes\n",
    "\n",
    "<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color: #e6f9ff; font-size:100%;\">\n",
    "   \n",
    "* V11 (CV 1-fold: 90.76 / LB: 97.66)\n",
    "    * Create feature processing per dataset inside the  class FeatureProcessorClass\n",
    "    * Renaming of the features per dataset\n",
    "    * Remove latitude/longitude columns for model\n",
    "    * Add mean_price_per_mwh_gas as feature\n",
    "\n",
    "\n",
    "* V21 (CV 1-fold: 78.99 / LB: 86.43)\n",
    "    * Add revealed_target lags from 2 to 7 days ago - inspired from [[Enefit] Baseline + cross-validation ‚òÄÔ∏è](https://www.kaggle.com/code/vincentschuler/enefit-baseline-cross-validation)\n",
    "    * Use custom N_days_lags to specify the max number of revealed_target day lags\n",
    "\n",
    "    \n",
    "* V23 (CV 1-fold: 72.96 / LB: 83.79)\n",
    "    * Map latitude & longitude for each county, using code from [mapping locations and county codes\n",
    "](https://www.kaggle.com/code/fabiendaniel/mapping-locations-and-county-codes)\n",
    "    * *historical_weather* and *forecast_weather* group by county too, and specify aggegate statistics  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "> üìå**Note**: If you liked or forked this notebook, please consider upvoting ‚¨ÜÔ∏è‚¨ÜÔ∏è It encourages to keep posting relevant content\n",
    "\n",
    "<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color: #e6f9ff; font-size:100%; \">\n",
    "    \n",
    "This notebook covers the following:\n",
    "* Pre-processing of the different datasets \n",
    "* Basic merging of the datasets \n",
    "* Simple feature engineering\n",
    "* XGBoost starter model \n",
    "* Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Description\n",
    "<img src =\"https://www.energy.gov/sites/default/files/styles/full_article_width/public/Prosumer-Blog%20sans%20money-%201200%20x%20630-01_0.png?itok=2a3YSkUb\" width=600>\n",
    "\n",
    "> üìå**Note**:  Energy prosumers are individuals, businesses, or organizations that both consume and produce energy. This concept represents a shift from the traditional model where consumers simply purchase energy from utilities and rely on centralized power generation sources. Energy prosumers are actively involved in the energy ecosystem by generating their own electricity, typically through renewable energy sources like solar panels (or wind turbines, small-scale hydropower etc.). They also consume energy from the grid when their own generation is insufficient to meet their needs\n",
    "\n",
    "<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color: #e6f9ff; font-size:100%; \">\n",
    "    \n",
    "* The number of prosumers is rapidly increasing, associated with higher energy imbalance - increased operational costs, potential grid instability, and inefficient use of energy resources.\n",
    "* The goal of the competition is to create an energy prediction model of prosumers to reduce energy imbalance costs\n",
    "* If solved, it would reduce the imbalance costs, improve the reliability of the grid, and make the integration of prosumers into the energy system more efficient and sustainable.\n",
    "*  Moreover, it could potentially incentivize more consumers to become prosumers and thus promote renewable energy production and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "> üìå**Note**:  Your challenge in this competition is to predict the amount of electricity produced and consumed by Estonian energy customers who have installed solar panels. You'll have access to weather data, the relevant energy prices, and records of the installed photovoltaic capacity. <br> <br>\n",
    "This is a forecasting competition using the time series API. The private leaderboard will be determined using real data gathered after the submission period closes.\n",
    "\n",
    "<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color: #e6f9ff; font-size:100%; \">\n",
    "\n",
    "## Files\n",
    "\n",
    "**train.csv**\n",
    "\n",
    "- `county` - An ID code for the county.\n",
    "- `is_business` - Boolean for whether or not the prosumer is a business.\n",
    "- `product_type` - ID code with the following mapping of codes to contract types: `{0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}`.\n",
    "- `target` - The consumption or production amount for the relevant segment for the hour. The segments are defined by the `county`, `is_business`, and `product_type`.\n",
    "- `is_consumption` - Boolean for whether or not this row's target is consumption or production.\n",
    "- `datetime` - The Estonian time in EET (UTC+2) / EEST (UTC+3).\n",
    "- `data_block_id` - All rows sharing the same `data_block_id` will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather `data_block_id` for predictions made on October 31st is 100 then the historic weather `data_block_id` for October 31st will be 101 as the historic weather data is only actually available the next day.\n",
    "- `row_id` - A unique identifier for the row.\n",
    "- `prediction_unit_id` - A unique identifier for the `county`, `is_business`, and `product_type` combination. _New prediction units can appear or dissappear in the test set_.\n",
    "\n",
    "**gas\\_prices.csv**\n",
    "\n",
    "- `origin_date` - The date when the day-ahead prices became available.\n",
    "- `forecast_date` - The date when the forecast prices should be relevant.\n",
    "- `[lowest/highest]_price_per_mwh` - The lowest/highest price of natural gas that on the day ahead market that trading day, in Euros per megawatt hour equivalent.\n",
    "- `data_block_id`\n",
    "\n",
    "**client.csv**\n",
    "\n",
    "- `product_type`\n",
    "- `county` - An ID code for the county. See `county_id_to_name_map.json` for the mapping of ID codes to county names.\n",
    "- `eic_count` - The aggregated number of consumption points (EICs - European Identifier Code).\n",
    "- `installed_capacity` - Installed photovoltaic solar panel capacity in kilowatts.\n",
    "- `is_business` - Boolean for whether or not the prosumer is a business.\n",
    "- `date`\n",
    "- `data_block_id`\n",
    "\n",
    "**electricity\\_prices.csv**\n",
    "\n",
    "- `origin_date`\n",
    "- `forecast_date`\n",
    "- `euros_per_mwh` - The price of electricity on the day ahead markets in euros per megawatt hour.\n",
    "- `data_block_id`\n",
    "\n",
    "**forecast\\_weather.csv** Weather forecasts that would have been available at prediction time. Sourced from the [European Centre for Medium-Range Weather Forecasts](https://codes.ecmwf.int/grib/param-db/?filter=grib2).\n",
    "\n",
    "- `[latitude/longitude]` - The coordinates of the weather forecast.\n",
    "- `origin_datetime` - The timestamp of when the forecast was generated.\n",
    "- `hours_ahead` - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n",
    "- `temperature` - The air temperature at 2 meters above ground in degrees Celsius.\n",
    "- `dewpoint` - The dew point temperature at 2 meters above ground in degrees Celsius.\n",
    "- `cloudcover_[low/mid/high/total]` - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total.\n",
    "- `10_metre_[u/v]_wind_component` - The \\[eastward/northward\\] component of wind speed measured 10 meters above surface in meters per second.\n",
    "- `data_block_id`\n",
    "- `forecast_datetime` - The timestamp of the predicted weather. Generated from `origin_datetime` plus `hours_ahead`.\n",
    "- `direct_solar_radiation` - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the preceding hour, in watt-hours per square meter.\n",
    "- `surface_solar_radiation_downwards` - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, in watt-hours per square meter.\n",
    "- `snowfall` - Snowfall over the previous hour in units of meters of water equivalent.\n",
    "- `total_precipitation` - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the preceding hour, in units of meters.\n",
    "\n",
    "**historical\\_weather.csv** [Historic weather data](https://open-meteo.com/en/docs).\n",
    "\n",
    "- `datetime`\n",
    "- `temperature`\n",
    "- `dewpoint`\n",
    "- `rain` - Different from the forecast conventions. The rain from large scale weather systems of the preceding hour in millimeters.\n",
    "- `snowfall` - Different from the forecast conventions. Snowfall over the preceding hour in centimeters.\n",
    "- `surface_pressure` - The air pressure at surface in hectopascals.\n",
    "- `cloudcover_[low/mid/high/total]` - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n",
    "- `windspeed_10m` - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n",
    "- `winddirection_10m` - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n",
    "- `shortwave_radiation` - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n",
    "- `direct_solar_radiation`\n",
    "- `diffuse_radiation` - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n",
    "- `[latitude/longitude]` - The coordinates of the weather station.\n",
    "- `data_block_id`\n",
    "\n",
    "**public\\_timeseries\\_testing\\_util.py** An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it.\n",
    "\n",
    "**example\\_test\\_files/** Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three `data_block_ids` are repeats of the last three `data_block_ids` in the train set.\n",
    "\n",
    "**example\\_test\\_files/sample\\_submission.csv** A valid sample submission, delivered by the API. See [this notebook](https://www.kaggle.com/code/sohier/enefit-basic-submission-demo/notebook) for a very simple example of how to use the sample submission.\n",
    "\n",
    "**example\\_test\\_files/revealed\\_targets.csv** The actual target values, served with a lag of one day.\n",
    "\n",
    "**enefit/** Files that enable the API. Expect the API to deliver all rows in under 15 minutes and to reserve less than 0.5 GB of memory. The copy of the API that you can download serves the data from **example\\_test\\_files/**. You must make predictions for those dates in order to advance the API but those predictions are not scored. Expect to see roughly three months of data delivered initially and up to ten months of data by the end of the forecasting period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:06:50.555500Z",
     "iopub.status.busy": "2024-02-06T21:06:50.555158Z",
     "iopub.status.idle": "2024-02-06T21:06:57.819898Z",
     "shell.execute_reply": "2024-02-06T21:06:57.819064Z",
     "shell.execute_reply.started": "2024-02-06T21:06:50.555470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from colorama import Fore, Style, init;\n",
    "\n",
    "# Modeling\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "\n",
    "# Geolocation\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Options\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:06:57.821463Z",
     "iopub.status.busy": "2024-02-06T21:06:57.820863Z",
     "iopub.status.idle": "2024-02-06T21:06:57.825956Z",
     "shell.execute_reply": "2024-02-06T21:06:57.825021Z",
     "shell.execute_reply.started": "2024-02-06T21:06:57.821434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False # False/True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:06:57.827847Z",
     "iopub.status.busy": "2024-02-06T21:06:57.827578Z",
     "iopub.status.idle": "2024-02-06T21:06:57.859859Z",
     "shell.execute_reply": "2024-02-06T21:06:57.859122Z",
     "shell.execute_reply.started": "2024-02-06T21:06:57.827822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPU or CPU use for model\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:06:57.862372Z",
     "iopub.status.busy": "2024-02-06T21:06:57.862080Z",
     "iopub.status.idle": "2024-02-06T21:06:57.879463Z",
     "shell.execute_reply": "2024-02-06T21:06:57.878670Z",
     "shell.execute_reply.started": "2024-02-06T21:06:57.862325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def display_df(df, name):\n",
    "    '''Display df shape and first row '''\n",
    "    PrintColor(text = f'{name} data has {df.shape[0]} rows and {df.shape[1]} columns. \\n ===> First row:')\n",
    "    display(df.head(1))\n",
    "\n",
    "# Color printing    \n",
    "def PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n",
    "    '''Prints color outputs using colorama of a text string'''\n",
    "    print(style + color + text + Style.RESET_ALL); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:06:57.880702Z",
     "iopub.status.busy": "2024-02-06T21:06:57.880452Z",
     "iopub.status.idle": "2024-02-06T21:07:20.153963Z",
     "shell.execute_reply": "2024-02-06T21:07:20.152830Z",
     "shell.execute_reply.started": "2024-02-06T21:06:57.880679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# Read CSVs and parse relevant date columns\n",
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "client = pd.read_csv(DATA_DIR + \"client.csv\")\n",
    "historical_weather = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\n",
    "forecast_weather = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\n",
    "electricity = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\n",
    "gas = pd.read_csv(DATA_DIR + \"gas_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:20.158808Z",
     "iopub.status.busy": "2024-02-06T21:07:20.158190Z",
     "iopub.status.idle": "2024-02-06T21:07:20.198678Z",
     "shell.execute_reply": "2024-02-06T21:07:20.197548Z",
     "shell.execute_reply.started": "2024-02-06T21:07:20.158779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Location from https://www.kaggle.com/datasets/michaelo/fabiendaniels-mapping-locations-and-county-codes/data\n",
    "location = (pd.read_csv(\"data/county_lon_lats.csv\")\n",
    "            .drop(columns = [\"Unnamed: 0\"])\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:20.200379Z",
     "iopub.status.busy": "2024-02-06T21:07:20.199932Z",
     "iopub.status.idle": "2024-02-06T21:07:20.503678Z",
     "shell.execute_reply": "2024-02-06T21:07:20.502678Z",
     "shell.execute_reply.started": "2024-02-06T21:07:20.200321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display_df(train, 'train')\n",
    "display_df(client, 'client')\n",
    "display_df(historical_weather, 'historical weather')\n",
    "display_df(forecast_weather, 'forecast weather')\n",
    "display_df(electricity, 'electricity prices')\n",
    "display_df(gas, 'gas prices')\n",
    "display_df(location, 'location data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:20.505904Z",
     "iopub.status.busy": "2024-02-06T21:07:20.505471Z",
     "iopub.status.idle": "2024-02-06T21:07:20.558031Z",
     "shell.execute_reply": "2024-02-06T21:07:20.557062Z",
     "shell.execute_reply.started": "2024-02-06T21:07:20.505867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# See county codes\n",
    "with open(DATA_DIR + 'county_id_to_name_map.json') as f:\n",
    "    county_codes = json.load(f)\n",
    "pd.DataFrame(county_codes, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:20.559309Z",
     "iopub.status.busy": "2024-02-06T21:07:20.559040Z",
     "iopub.status.idle": "2024-02-06T21:07:20.628283Z",
     "shell.execute_reply": "2024-02-06T21:07:20.627087Z",
     "shell.execute_reply.started": "2024-02-06T21:07:20.559285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(train[train['is_consumption']==0].target.describe(percentiles = [0, 0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999])).round(2).T\n",
    "# pd.DataFrame(train[train['is_consumption']==1].target.describe(percentiles = [0, 0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999])).round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:20.631677Z",
     "iopub.status.busy": "2024-02-06T21:07:20.631259Z",
     "iopub.status.idle": "2024-02-06T21:07:20.729593Z",
     "shell.execute_reply": "2024-02-06T21:07:20.728279Z",
     "shell.execute_reply.started": "2024-02-06T21:07:20.631649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureProcessorClass():\n",
    "    def __init__(self):         \n",
    "        # Columns to join on for the different datasets\n",
    "        self.weather_join = ['datetime', 'county', 'data_block_id']\n",
    "        self.gas_join = ['data_block_id']\n",
    "        self.electricity_join = ['datetime', 'data_block_id']\n",
    "        self.client_join = ['county', 'is_business', 'product_type', 'data_block_id']\n",
    "        \n",
    "        # Columns of latitude & longitude\n",
    "        self.lat_lon_columns = ['latitude', 'longitude']\n",
    "        \n",
    "        # Aggregate stats \n",
    "        self.agg_stats = ['mean'] #, 'min', 'max', 'std', 'median']\n",
    "        \n",
    "        # Categorical columns (specify for XGBoost)\n",
    "        self.category_columns = ['county', 'is_business', 'product_type', 'is_consumption', 'data_block_id']\n",
    "\n",
    "    def create_new_column_names(self, df, suffix, columns_no_change):\n",
    "        '''Change column names by given suffix, keep columns_no_change, and return back the data'''\n",
    "        df.columns = [col + suffix \n",
    "                      if col not in columns_no_change\n",
    "                      else col\n",
    "                      for col in df.columns\n",
    "                      ]\n",
    "        return df \n",
    "\n",
    "    def flatten_multi_index_columns(self, df):\n",
    "        df.columns = ['_'.join([col for col in multi_col if len(col)>0]) \n",
    "                      for multi_col in df.columns]\n",
    "        return df\n",
    "    \n",
    "    def create_data_features(self, data):\n",
    "        '''üìäCreate features for main data (test or train) setüìä'''\n",
    "        # To datetime\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "        \n",
    "        # Time period features\n",
    "        data['date'] = data['datetime'].dt.normalize()\n",
    "        data['year'] = data['datetime'].dt.year\n",
    "        data['quarter'] = data['datetime'].dt.quarter\n",
    "        data['month'] = data['datetime'].dt.month\n",
    "        data['week'] = data['datetime'].dt.isocalendar().week\n",
    "        data['hour'] = data['datetime'].dt.hour\n",
    "        \n",
    "        # Day features\n",
    "        data['day_of_year'] = data['datetime'].dt.day_of_year\n",
    "        data['day_of_month']  = data['datetime'].dt.day\n",
    "        data['day_of_week'] = data['datetime'].dt.day_of_week\n",
    "        return data\n",
    "\n",
    "    def create_client_features(self, client):\n",
    "        '''üíº Create client features üíº'''\n",
    "        # Modify column names - specify suffix\n",
    "        client = self.create_new_column_names(client, \n",
    "                                           suffix='_client',\n",
    "                                           columns_no_change = self.client_join\n",
    "                                          )       \n",
    "        return client\n",
    "    \n",
    "    def create_historical_weather_features(self, historical_weather):\n",
    "        '''‚åõüå§Ô∏è Create historical weather features üå§Ô∏è‚åõ'''\n",
    "        \n",
    "        # To datetime\n",
    "        historical_weather['datetime'] = pd.to_datetime(historical_weather['datetime'])\n",
    "        \n",
    "        # Add county\n",
    "        historical_weather[self.lat_lon_columns] = historical_weather[self.lat_lon_columns].astype(float).round(1)\n",
    "        historical_weather = historical_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n",
    "\n",
    "        # Modify column names - specify suffix\n",
    "        historical_weather = self.create_new_column_names(historical_weather,\n",
    "                                                          suffix='_h',\n",
    "                                                          columns_no_change = self.lat_lon_columns + self.weather_join\n",
    "                                                          ) \n",
    "        \n",
    "        # Group by & calculate aggregate stats \n",
    "        agg_columns = [col for col in historical_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n",
    "        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n",
    "        historical_weather = historical_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n",
    "        \n",
    "        # Flatten the multi column aggregates\n",
    "        historical_weather = self.flatten_multi_index_columns(historical_weather) \n",
    "        \n",
    "        # Test set has 1 day offset for hour<11 and 2 day offset for hour>11\n",
    "        historical_weather['hour_h'] = historical_weather['datetime'].dt.hour\n",
    "        historical_weather['datetime'] = (historical_weather\n",
    "                                               .apply(lambda x: \n",
    "                                                      x['datetime'] + pd.DateOffset(1) \n",
    "                                                      if x['hour_h']< 11 \n",
    "                                                      else x['datetime'] + pd.DateOffset(2),\n",
    "                                                      axis=1)\n",
    "                                              )\n",
    "        \n",
    "        return historical_weather\n",
    "    \n",
    "    def create_forecast_weather_features(self, forecast_weather):\n",
    "        '''üîÆüå§Ô∏è Create forecast weather features üå§Ô∏èüîÆ'''\n",
    "        \n",
    "        # Rename column and drop\n",
    "        forecast_weather = (forecast_weather\n",
    "                            .rename(columns = {'forecast_datetime': 'datetime'})\n",
    "                            .drop(columns = 'origin_datetime') # not needed\n",
    "                           )\n",
    "        \n",
    "        # To datetime\n",
    "        forecast_weather['datetime'] = (pd.to_datetime(forecast_weather['datetime'])\n",
    "                                        .dt\n",
    "                                        .tz_localize(None)\n",
    "                                       )\n",
    "\n",
    "        # Add county\n",
    "        forecast_weather[self.lat_lon_columns] = forecast_weather[self.lat_lon_columns].astype(float).round(1)\n",
    "        forecast_weather = forecast_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n",
    "        \n",
    "        # Modify column names - specify suffix\n",
    "        forecast_weather = self.create_new_column_names(forecast_weather,\n",
    "                                                        suffix='_f',\n",
    "                                                        columns_no_change = self.lat_lon_columns + self.weather_join\n",
    "                                                        ) \n",
    "        \n",
    "        # Group by & calculate aggregate stats \n",
    "        agg_columns = [col for col in forecast_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n",
    "        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n",
    "        forecast_weather = forecast_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n",
    "        \n",
    "        # Flatten the multi column aggregates\n",
    "        forecast_weather = self.flatten_multi_index_columns(forecast_weather)     \n",
    "        return forecast_weather\n",
    "\n",
    "    def create_electricity_features(self, electricity):\n",
    "        '''‚ö° Create electricity prices features ‚ö°'''\n",
    "        # To datetime\n",
    "        electricity['forecast_date'] = pd.to_datetime(electricity['forecast_date'])\n",
    "        \n",
    "        # Test set has 1 day offset\n",
    "        electricity['datetime'] = electricity['forecast_date'] + pd.DateOffset(1)\n",
    "        \n",
    "        # Modify column names - specify suffix\n",
    "        electricity = self.create_new_column_names(electricity, \n",
    "                                                   suffix='_electricity',\n",
    "                                                   columns_no_change = self.electricity_join\n",
    "                                                  )             \n",
    "        return electricity\n",
    "\n",
    "    def create_gas_features(self, gas):\n",
    "        '''‚õΩ Create gas prices features ‚õΩ'''\n",
    "        # Mean gas price\n",
    "        gas['mean_price_per_mwh'] = (gas['lowest_price_per_mwh'] + gas['highest_price_per_mwh'])/2\n",
    "        \n",
    "        # Modify column names - specify suffix\n",
    "        gas = self.create_new_column_names(gas, \n",
    "                                           suffix='_gas',\n",
    "                                           columns_no_change = self.gas_join\n",
    "                                          )       \n",
    "        return gas\n",
    "    \n",
    "    def __call__(self, data, client, historical_weather, forecast_weather, electricity, gas):\n",
    "        '''Processing of features from all datasets, merge together and return features for dataframe df '''\n",
    "        # Create features for relevant dataset\n",
    "        data = self.create_data_features(data)\n",
    "        client = self.create_client_features(client)\n",
    "        historical_weather = self.create_historical_weather_features(historical_weather)\n",
    "        forecast_weather = self.create_forecast_weather_features(forecast_weather)\n",
    "        electricity = self.create_electricity_features(electricity)\n",
    "        gas = self.create_gas_features(gas)\n",
    "        \n",
    "        # üîó Merge all datasets into one df üîó\n",
    "        df = data.merge(client, how='left', on = self.client_join)\n",
    "        df = df.merge(historical_weather, how='left', on = self.weather_join)\n",
    "        df = df.merge(forecast_weather, how='left', on = self.weather_join)\n",
    "        df = df.merge(electricity, how='left', on = self.electricity_join)\n",
    "        df = df.merge(gas, how='left', on = self.gas_join)\n",
    "        \n",
    "        # Change columns to categorical for XGBoost\n",
    "        df[self.category_columns] = df[self.category_columns].astype('category')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:20.731806Z",
     "iopub.status.busy": "2024-02-06T21:07:20.731407Z",
     "iopub.status.idle": "2024-02-06T21:07:20.845366Z",
     "shell.execute_reply": "2024-02-06T21:07:20.843942Z",
     "shell.execute_reply.started": "2024-02-06T21:07:20.731769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_revealed_targets_train(data, N_day_lags):\n",
    "    '''üéØ Create past revealed_targets for train set based on number of day lags N_day_lags üéØ '''    \n",
    "    original_datetime = data['datetime']\n",
    "    revealed_targets = data[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy()\n",
    "    \n",
    "    # Create revealed targets for all day lags\n",
    "    for day_lag in range(2, N_day_lags+1):\n",
    "        revealed_targets['datetime'] = original_datetime + pd.DateOffset(day_lag)\n",
    "        data = data.merge(revealed_targets, \n",
    "                          how='left', \n",
    "                          on = ['datetime', 'prediction_unit_id', 'is_consumption'],\n",
    "                          suffixes = ('', f'_{day_lag}_days_ago')\n",
    "                         )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:20.847581Z",
     "iopub.status.busy": "2024-02-06T21:07:20.847181Z",
     "iopub.status.idle": "2024-02-06T21:07:52.912047Z",
     "shell.execute_reply": "2024-02-06T21:07:52.911043Z",
     "shell.execute_reply.started": "2024-02-06T21:07:20.847552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create all features\n",
    "N_day_lags = 15 # Specify how many days we want to go back (at least 2)\n",
    "\n",
    "FeatureProcessor = FeatureProcessorClass()\n",
    "\n",
    "data = FeatureProcessor(data = train.copy(),\n",
    "                      client = client.copy(),\n",
    "                      historical_weather = historical_weather.copy(),\n",
    "                      forecast_weather = forecast_weather.copy(),\n",
    "                      electricity = electricity.copy(),\n",
    "                      gas = gas.copy(),\n",
    "                     )\n",
    "\n",
    "df = create_revealed_targets_train(data.copy(), \n",
    "                                  N_day_lags = N_day_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:52.915996Z",
     "iopub.status.busy": "2024-02-06T21:07:52.915696Z",
     "iopub.status.idle": "2024-02-06T21:07:53.034792Z",
     "shell.execute_reply": "2024-02-06T21:07:53.033862Z",
     "shell.execute_reply.started": "2024-02-06T21:07:52.915962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost single fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:53.036245Z",
     "iopub.status.busy": "2024-02-06T21:07:53.035974Z",
     "iopub.status.idle": "2024-02-06T21:07:54.907702Z",
     "shell.execute_reply": "2024-02-06T21:07:54.906715Z",
     "shell.execute_reply.started": "2024-02-06T21:07:53.036221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#### Create single fold split ######\n",
    "# Remove empty target row\n",
    "target = 'target'\n",
    "df = df[df[target].notnull()].reset_index(drop=True)\n",
    "\n",
    "train_block_id = list(range(0, 600)) \n",
    "\n",
    "tr = df[df['data_block_id'].isin(train_block_id)] # first 600 data_block_ids used for training\n",
    "val = df[~df['data_block_id'].isin(train_block_id)] # rest data_block_ids used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:54.909185Z",
     "iopub.status.busy": "2024-02-06T21:07:54.908909Z",
     "iopub.status.idle": "2024-02-06T21:07:54.915229Z",
     "shell.execute_reply": "2024-02-06T21:07:54.914272Z",
     "shell.execute_reply.started": "2024-02-06T21:07:54.909160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove columns for features\n",
    "no_features = ['date', \n",
    "                'latitude', \n",
    "                'longitude', \n",
    "                'data_block_id', \n",
    "                'row_id',\n",
    "                'hours_ahead',\n",
    "                'hour_h',\n",
    "               ]\n",
    "\n",
    "remove_columns = [col for col in df.columns for no_feature in no_features if no_feature in col]\n",
    "remove_columns.append(target)\n",
    "features = [col for col in df.columns if col not in remove_columns]\n",
    "PrintColor(f'There are {len(features)} features: {features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:54.916885Z",
     "iopub.status.busy": "2024-02-06T21:07:54.916626Z",
     "iopub.status.idle": "2024-02-06T21:07:54.931106Z",
     "shell.execute_reply": "2024-02-06T21:07:54.930152Z",
     "shell.execute_reply.started": "2024-02-06T21:07:54.916862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBRegressor(\n",
    "                        device = device,\n",
    "                        enable_categorical=True,\n",
    "                        objective = 'reg:absoluteerror',\n",
    "                        n_estimators = 2 if DEBUG else 1500,\n",
    "                        early_stopping_rounds=100\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:07:54.932624Z",
     "iopub.status.busy": "2024-02-06T21:07:54.932291Z",
     "iopub.status.idle": "2024-02-06T21:08:41.538834Z",
     "shell.execute_reply": "2024-02-06T21:08:41.537974Z",
     "shell.execute_reply.started": "2024-02-06T21:07:54.932598Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X = tr[features], \n",
    "        y = tr[target], \n",
    "        eval_set = [(tr[features], tr[target]), (val[features], val[target])], \n",
    "        verbose=True #False #True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:08:41.540297Z",
     "iopub.status.busy": "2024-02-06T21:08:41.540035Z",
     "iopub.status.idle": "2024-02-06T21:08:41.545658Z",
     "shell.execute_reply": "2024-02-06T21:08:41.544657Z",
     "shell.execute_reply.started": "2024-02-06T21:08:41.540272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PrintColor(f'Early stopping on best iteration #{clf.best_iteration} with MAE error on validation set of {clf.best_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:08:41.547032Z",
     "iopub.status.busy": "2024-02-06T21:08:41.546777Z",
     "iopub.status.idle": "2024-02-06T21:08:41.861192Z",
     "shell.execute_reply": "2024-02-06T21:08:41.860259Z",
     "shell.execute_reply.started": "2024-02-06T21:08:41.547008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot RMSE\n",
    "results = clf.evals_result()\n",
    "train_mae, val_mae = results[\"validation_0\"][\"mae\"], results[\"validation_1\"][\"mae\"]\n",
    "x_values = range(0, len(train_mae))\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(x_values, train_mae, label=\"Train MAE\")\n",
    "ax.plot(x_values, val_mae, label=\"Validation MAE\")\n",
    "ax.legend()\n",
    "plt.ylabel(\"MAE Loss\")\n",
    "plt.title(\"XGBoost MAE Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:08:41.862603Z",
     "iopub.status.busy": "2024-02-06T21:08:41.862302Z",
     "iopub.status.idle": "2024-02-06T21:08:42.492286Z",
     "shell.execute_reply": "2024-02-06T21:08:42.491382Z",
     "shell.execute_reply.started": "2024-02-06T21:08:41.862576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TOP = 20\n",
    "importance_data = pd.DataFrame({'name': clf.feature_names_in_, 'importance': clf.feature_importances_})\n",
    "importance_data = importance_data.sort_values(by='importance', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "sns.barplot(data=importance_data[:TOP],\n",
    "            x = 'importance',\n",
    "            y = 'name'\n",
    "        )\n",
    "patches = ax.patches\n",
    "count = 0\n",
    "for patch in patches:\n",
    "    height = patch.get_height() \n",
    "    width = patch.get_width()\n",
    "    perc = 100*importance_data['importance'].iloc[count]#100*width/len(importance_data)\n",
    "    ax.text(width, patch.get_y() + height/2, f'{perc:.1f}%')\n",
    "    count+=1\n",
    "    \n",
    "plt.title(f'The top {TOP} features sorted by importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:08:42.493810Z",
     "iopub.status.busy": "2024-02-06T21:08:42.493526Z",
     "iopub.status.idle": "2024-02-06T21:08:42.500719Z",
     "shell.execute_reply": "2024-02-06T21:08:42.499849Z",
     "shell.execute_reply.started": "2024-02-06T21:08:42.493783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "importance_data[importance_data['importance']<0.0005].name.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:08:42.502106Z",
     "iopub.status.busy": "2024-02-06T21:08:42.501848Z",
     "iopub.status.idle": "2024-02-06T21:08:42.514799Z",
     "shell.execute_reply": "2024-02-06T21:08:42.513945Z",
     "shell.execute_reply.started": "2024-02-06T21:08:42.502084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_revealed_targets_test(data, previous_revealed_targets, N_day_lags):\n",
    "    '''üéØ Create new test data based on previous_revealed_targets and N_day_lags üéØ ''' \n",
    "    for count, revealed_targets in enumerate(previous_revealed_targets) :\n",
    "        day_lag = count + 2\n",
    "        \n",
    "        # Get hour\n",
    "        revealed_targets['hour'] = pd.to_datetime(revealed_targets['datetime']).dt.hour\n",
    "        \n",
    "        # Select columns and rename target\n",
    "        revealed_targets = revealed_targets[['hour', 'prediction_unit_id', 'is_consumption', 'target']]\n",
    "        revealed_targets = revealed_targets.rename(columns = {\"target\" : f\"target_{day_lag}_days_ago\"})\n",
    "        \n",
    "        \n",
    "        # Add past revealed targets\n",
    "        data = pd.merge(data,\n",
    "                        revealed_targets,\n",
    "                        how = 'left',\n",
    "                        on = ['hour', 'prediction_unit_id', 'is_consumption'],\n",
    "                       )\n",
    "        \n",
    "    # If revealed_target_columns not available, replace by nan\n",
    "    all_revealed_columns = [f\"target_{day_lag}_days_ago\" for day_lag in range(2, N_day_lags+1)]\n",
    "    missing_columns = list(set(all_revealed_columns) - set(data.columns))\n",
    "    data[missing_columns] = np.nan \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:08:42.516013Z",
     "iopub.status.busy": "2024-02-06T21:08:42.515766Z",
     "iopub.status.idle": "2024-02-06T21:08:42.550855Z",
     "shell.execute_reply": "2024-02-06T21:08:42.550029Z",
     "shell.execute_reply.started": "2024-02-06T21:08:42.515991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import enefit\n",
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:08:42.552095Z",
     "iopub.status.busy": "2024-02-06T21:08:42.551863Z",
     "iopub.status.idle": "2024-02-06T21:08:42.556767Z",
     "shell.execute_reply": "2024-02-06T21:08:42.555771Z",
     "shell.execute_reply.started": "2024-02-06T21:08:42.552074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reload enefit environment (only in debug mode, otherwise the submission will fail)\n",
    "if DEBUG:\n",
    "    enefit.make_env.__called__ = False\n",
    "    type(env)._state = type(type(env)._state).__dict__['INIT']\n",
    "    iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:08:42.558125Z",
     "iopub.status.busy": "2024-02-06T21:08:42.557864Z",
     "iopub.status.idle": "2024-02-06T21:08:43.614100Z",
     "shell.execute_reply": "2024-02-06T21:08:43.613190Z",
     "shell.execute_reply.started": "2024-02-06T21:08:42.558102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List of target_revealed dataframes\n",
    "previous_revealed_targets = []\n",
    "\n",
    "for (test, \n",
    "     revealed_targets, \n",
    "     client_test, \n",
    "     historical_weather_test,\n",
    "     forecast_weather_test, \n",
    "     electricity_test, \n",
    "     gas_test, \n",
    "     sample_prediction) in iter_test:\n",
    "    \n",
    "    # Rename test set to make consistent with train\n",
    "    test = test.rename(columns = {'prediction_datetime': 'datetime'})\n",
    "\n",
    "    # Initiate column data_block_id with default value to join on\n",
    "    id_column = 'data_block_id' \n",
    "    \n",
    "    test[id_column] = 0\n",
    "    gas_test[id_column] = 0\n",
    "    electricity_test[id_column] = 0\n",
    "    historical_weather_test[id_column] = 0\n",
    "    forecast_weather_test[id_column] = 0\n",
    "    client_test[id_column] = 0\n",
    "    revealed_targets[id_column] = 0\n",
    "    \n",
    "    data_test = FeatureProcessor(\n",
    "                               data = test,\n",
    "                               client = client_test, \n",
    "                               historical_weather = historical_weather_test,\n",
    "                               forecast_weather = forecast_weather_test, \n",
    "                               electricity = electricity_test, \n",
    "                               gas = gas_test\n",
    "                               )\n",
    "    \n",
    "    # Store revealed_targets\n",
    "    previous_revealed_targets.insert(0, revealed_targets)\n",
    "    \n",
    "    if len(previous_revealed_targets) == N_day_lags:\n",
    "        previous_revealed_targets.pop()\n",
    "    \n",
    "    # Add previous revealed targets\n",
    "    df_test = create_revealed_targets_test(data = data_test.copy(),\n",
    "                                           previous_revealed_targets = previous_revealed_targets.copy(),\n",
    "                                           N_day_lags = N_day_lags\n",
    "                                          )\n",
    "    \n",
    "    # Make prediction\n",
    "    X_test = df_test[features]\n",
    "    sample_prediction['target'] = clf.predict(X_test)\n",
    "    env.predict(sample_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "> üìå**Note**: If you liked or forked this notebook, please consider upvoting ‚¨ÜÔ∏è‚¨ÜÔ∏è It encourages to keep posting relevant content. Feedback is always welcome!!\n",
    "\n",
    "<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color: #e6f9ff; font-size:100%;\">\n",
    "\n",
    "* Create more rolling / lag features and make sure they are robust on the test set\n",
    "* Be creative with new feature engineering\n",
    "* Cross validation and hyperparameter tuning\n",
    "* Choose other models e.g. CatBoost, LGBM, Neural Networks (Transformers?) and ensemble  \n",
    "* Alternative merging, not sure the merging I used is the most correct!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    },
    {
     "datasetId": 3976011,
     "sourceId": 6924580,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3933894,
     "sourceId": 7140010,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Energy Prosumers (uv)",
   "language": "python",
   "name": "energy-prosumers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
